1. Convolutional filter receptive field

2. Run the Tensorflow conv net
    3 layers
    2 convolutional, 1 fully connected

    Relu is used on hidden nodes

    Loss function is softmax_cross_entropy_with_logits

    Initial implementation:
    Batch loss at step 1500: 0.089476
    Batch training accuracy: 100.0%
    Validation accuracy: 67.8%
    Full train accuracy: 94.0%
    Finished training. Total time taken: 35.7028548717

    Severely overfitting.

3. Add pooling layers

    Filter = 2, Stride = 1:
        Validation accuracy: 70.1%
        Full train accuracy: 92.3%
    Filter = 2, Stride = 2:
        Validation accuracy: 64.4%
        Full train accuracy: 79.1%
    Filter = 2, Stride = 3:
        Validation accuracy: 50.6%
        Full train accuracy: 66.5%
    Filter = 2, Stride = 4:
        Validation accuracy: 51.7%
        Full train accuracy: 60.4%

    Filter = 1, Stride = 1:
        Validation accuracy: 67.8%
        Full train accuracy: 97.3%
    Filter = 2, Stride = 1:
        Validation accuracy: 67.8%
        Full train accuracy: 91.5%
    Filter = 3, Stride = 1:
        Validation accuracy: 67.8%
        Full train accuracy: 93.1%
        Validation accuracy: 75.9%
        Full train accuracy: 92.6%
        Validation accuracy: 66.7%
        Full train accuracy: 84.1%
    Filter = 4, Stride = 1:
        Validation accuracy: 66.7%
        Full train accuracy: 84.3%
Really high variance. Filter size does not seem to matter. Use filter size of 2. Stride does matter. Use stride 1.

4. Regularize your network!

	Dropout: 
	P = 0.8 
		Validation accuracy: 63.2%
		Full train accuracy: 83.0%
	P = 0.7
		Validation accuracy: 69.0%
		Full train accuracy: 84.9%
	P = 0.6
		Validation accuracy: 65.5%
		Full train accuracy: 84.9%
	P = .5
		Validation accuracy: 58.6%
		Full train accuracy: 72.0%

    Only Weight Penalty:
    weight_penalty = 0.0
        Validation accuracy: 65.5%
        Full train accuracy: 90.7%
        Validation accuracy: 67.8%
        Full train accuracy: 94.5%
    weight_penalty = 0.05
        Validation accuracy: 67.8%
        Full train accuracy: 90.4% 
        Validation accuracy: 71.3%
        Full train accuracy: 90.9% 
        Validation accuracy: 64.4%
        Full train accuracy: 85.7%
    weight_penalty = 0.075
        Validation accuracy: 65.5%
        Full train accuracy: 83.5%
    weight_penalty = 0.1
        Validation accuracy: 71.3%
        Full train accuracy: 78.6%
        Validation accuracy: 64.4%
        Full train accuracy: 81.0%
        Validation accuracy: 60.9%
        Full train accuracy: 75.0%
    weight_penalty = 0.15
        Validation accuracy: 60.9%
        Full train accuracy: 76.9%
        Validation accuracy: 65.5%
        Full train accuracy: 83.5%
    weight_penalty = 0.2
        Validation accuracy: 59.8%
        Full train accuracy: 68.7%
    weight_penalty = 0.3
        Validation accuracy: 55.2%
        Full train accuracy: 60.7%
    weight_penalty = 0.5
        Validation accuracy: 54.0%
        Full train accuracy: 60.4%
    weight_penalty = 1.0
        Validation accuracy: 37.9%
        Full train accuracy: 41.5%

    weight_penalty is best at about 0.05. Not very effective.

    Only Data Augmentation:
        Validation accuracy: 70.1%
        Full train accuracy: 98.8%

    Try stopping at 3500.
        Dropout = 0.7, weight_penalty = 0.05
            Validation accuracy: 66.7%
            Full train accuracy: 83.2%

    Try stopping at 6000.
        Dropout = 0.7, weight_penalty = 0.05
            Validation accuracy: 67.8%
            Full train accuracy: 95.4%
        Dropout = 0.5, weight_penalty = 0.05
            Validation accuracy: 64.4%
            Full train accuracy: 87.2%


    Only Early Stopping:

5. Experiment with your architecture

6. Optimize your architecture

7. Test your final architecture on variations of the data