\documentclass[10pt,twoside]{article}

\usepackage[margin=.8in]{geometry}
\usepackage{epsfig}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\title{6.867 Machine Learning  \subtitle{Homework 3} }

\maketitle

% **************************************************************************************************
 % Problem 1
% **************************************************************************************************

\section{\uppercase{Neural Networks}}

\noindent In this paper, we explore the performance of Neural Networks on different classification tasks. Neural Networks were invented to try to model how the neurons work in the human brain. A neural network consists of multiple layers of neurons, each of which takes a linear combination of its inputs and uses that to compute an output. 


\subsection{Implementation}

In our implementation of a neural network, we represented the the weights of each layer as an $m$ x $n$ matrix. Where $m$ is the size of the previous layer and $n$ is the size of the current layer. Each column in the matrix represented the weights for a single neuron in the layer. The bias term for each neuron was captured in a separate $n$ x 1 vector. We used ReLU activation functions for the neurons in the hidden layers and a softmax activation function for the last layer. The loss function we used was the cross entropy loss function, which is $\sum _i -y_i log(f(z)_i) $. Where $f(z)$ is the output of the softmax function. We used stochastic gradient descent to train the parameters of the network, which in this case are the weights and biases of each layer. Computing the gradient of a neural network with respect to the weights is a very complicated operation. Fortunately, the derivative can be found using backpropagation. Backpropagation finds the gradient of the loss function with respect to each layer by propagating the error back through the network. The first step in this is computing the graident of the loss function with respect to the output layer. A convenient property of the combination of a softmax output layer and the cross entropy function is that the derivative of the cost function with respect to the softmax function simplifies to $f(z)-t$ where $f(z)$ is the output of the softmax and $t$ is the target vector. 
\noindent smaller.

\subsection{Section 2}


\subsubsection{sub sub sub section}


\subsection{Install Latex}

% **************************************************************************************************
 % Problem 2
% **************************************************************************************************

\section{\uppercase{Convolutional Neural Networks}}

\noindent Stuff

\subsection{Artiist}

\noindent 

\subsection{2D Dataset Results}

Some table. 
\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
  & Training Error & Validation Error & Test Error \\ [0.5ex] 
 \hline\hline
 Dataset 1 & 0 & 0 & 0 \\ 
 \hline
 Dataset 2 & 0.1775 & 0.18 & 0.195 \\
 \hline
 Dataset 3 & 0.02 & 0.03 & 0.045 \\
 \hline
 Dataset 4 & 0.3 & 0.305 & 0.3 \\
 \hline
\end{tabular}
\end{center}

\begin{figure}[h]
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data1_test_C1.png}
                \caption{Dataset 1, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data2_test_C1.png}
                \caption{Dataset 2, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data3_test_C1.png}
                \caption{Dataset 3, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data4_test_C1.png}
                \caption{Dataset 4, C=1}
        \end{subfigure}
        \caption{Linear SVM perfomance of different datasets. Datasets 1 and 3 are relatively linearly separable so this SVM performs well on them. Dataset 2 and 4 are definitely not linearly separable so this SVM is not the right model for the data.}\label{fig:animals}
\end{figure}

\subsection{Section}


\vfill
\end{document}

