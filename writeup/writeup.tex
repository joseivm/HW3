\documentclass[10pt,twoside]{article}

\usepackage[margin=.8in]{geometry}
\usepackage{epsfig}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\title{6.867 Machine Learning  \subtitle{Homework 3} }

\maketitle

% **************************************************************************************************
 % Problem 1
% **************************************************************************************************

\section{\uppercase{Neural Networks}}

\noindent In this paper, we explore the performance of Neural Networks on different classification tasks. Neural Networks were invented to try to model how the neurons work in the human brain. A neural network consists of multiple layers of neurons, each of which takes a linear combination of its inputs and uses that to compute an output. 


\subsection{Implementation}

In our implementation of a neural network, we represented the the weights of each layer as an $m$ x $n$ matrix. Where $m$ is the size of the previous layer and $n$ is the size of the current layer. Each column in the matrix represented the weights for a single neuron in the layer. The bias term for each neuron was captured in a separate $n$ x 1 vector. We used ReLU activation functions for the neurons in the hidden layers and a softmax activation function for the last layer. The loss function we used was the cross entropy loss function, which is 

$\sum _i -y_i log(f(z)_i) $. 

Where $f(z)$ is the output of the softmax function. We used stochastic gradient descent to train the parameters of the network, which in this case are the weights and biases of each layer. Computing the gradient of a neural network with respect to the weights is a very complicated operation. Fortunately, the derivative can be found using backpropagation. Backpropagation finds the gradient of the loss function with respect to each layer by propagating the error back through the network. The first step in this is computing the graident of the loss function with respect to the output layer. A convenient property of the combination of a softmax output layer and the cross entropy function is that the derivative of the cost function with respect to the softmax function simplifies to 

$f(z)-t$ 

where $f(z)$ is the output of the softmax and $t$ is the target vector. 

\subsection{Weight Initialization}

How the weights are initialized plays an important part in the training of neural networks. In the case where the hidden units have sigmoid functions, if the weights are too small then the nonlinearity of the sigmoid funciton is lost. This is because the derivative of the sigmoid function is approximately linear for inputs close to zero. If the weights are too large, then the derivative of the sigmoid approaches zero, which can negatively impact the gradient descent, causing it to converge to a suboptimal value, or to take too long to train. One approach to handle this problem is to initialize the weights to come from a Gaussian distribution with zero mean and variance of $1/m$ where $m$ is the number of units in the previous layer. For a linear neuron, we have that 
$y = \sum_i w_i x_i +b$. 
Calculating the variance gives us 
Var($y$) = Var($\sum_i w_i x_i$). 
Var($w_i x_i$) = $E(w_i)^2$Var($w_i$) + $E(w_i)^2$Var($x_i$) + Var($w_i$)Var($x_i$). 
If we assume that the inputs have zero mean, then the expression simplifies to 
Var($w_i x_i$) = Var($w_i$)Var($x_i$). 
Substituting into the originial equation gives us 
Var($y$) = $\sum_i Var(w_i)Var(x_i)$. 
Since they are identically distributed, we can rewrite it as 

Var($y$) = $m$Var($w_i$)Var($x_i$). 

Thus, setting Var($w_i$) = $1/m$ guarantees that the variance of the input will be equal to the variance of the output under these assumptions. This helps stabilize the signals that are propagated through the network. 

\subsection{Regularization}

Neural networks often overfit to the training data becuase they have many parameters. One way to stop the model from overfitting is to use regularization. Regularization aims to prevent overfitting by penalizing the size of the weight vector. This is implemented by adding the term $\lambda (\sum_j  \left \|w^{(j)}  \right \|^2_F )$ to the cost function, where $w^{j}$ represents the weights at layer $j$. Integrating this with our implementation only involved adding a $\lambda w$ term to the gradient of the loss function at every layer. Thus, the gradient of the loss function with respect to the weights at layer $i$ is $\frac{\partial C}{\partial w_i} + \lambd w_i$. The learning rule for stochastic gradient descent becomes $w \rightarrow (1- \eta \lambda)w - \eta \frac{\partial C}{\partial w} $ where $C$ is the cross entropy loss function.


\noindent smaller.

\subsection{Binary Classification}

We evaluated the performance of different network architectures on binary classification problems. The network architectures we tested were a single hidden layer with 3 units, a single hidden layer with 20 units, two hidden layers with 3 units, and two hidden layers with 20 units. We found that the number of layers in the network was the parameter most sensive to changes in the learning rate. for two layered networks, we found the optimal learning rate to be 0.01. For single layer networks, 
we found that the optimal learning rate was 1. To avoid overfitting, we used a validation set to evaluate the performance of the ne

\subsubsection{sub sub sub section}


\subsection{Install Latex}

% **************************************************************************************************
 % Problem 2
% **************************************************************************************************

\section{\uppercase{Convolutional Neural Networks}}

\noindent Convolutional neural networks(CNNs) are similar to feed forward neural networks but are better suited for the task of image recognition. Traditional neural networks use fully connected layers and hidden units to classify inputs. For even small 50x50 pixel images though, the squared factor of weights between layers that need to be trained quickly makes this problem untractable for any reasonably size dataset.  CNNs solves this problem by recycling parameters with a concept called filters. A filter is a $fxf$ matrix of weights that gets applied to every possible $fxf$ square in the $nxn$ input image to generate another $nxn$ output. Similar to NNs, these outputs then also undergo some nonlinear transformation. Many filters can be applied in parallel to the same input and the output of filters can be fed into the input of other filters. In this manner, CNNs boost a diverse set of possible architectures have made remarkable progress in image recognition.

In this section, we will explore the effect of different filter sizes, architectures, pooling, and regularization techniques on CNNs. The task was classifying images of paintings to their artist.

\subsection{Architecture}

\noindent Paintings were padded to 50x50 pixel RGB images and inputed as a three layer image. We fed these images through various CNNs implemented with TensorFlow. A CNN can consist of multiple layers and each layer takes three arguments, filter size, stride, and depth. Stride is how many pixels the filter shifts each time and is typically 1 or 2. Depth is the number of filters working in parallel at that layer. Note that if a layer has a depth of 8, a filter of size 3 in the next layer actually has $9*8$ nodes feeding into it. The receptive field of a node is defined as the number of pixels in the original image that contribute to it. For example, if a image was fed through a 5x5 filter layer and then a 3x3 filter layer, the receptive field of a node in the last layer would be 7x7. This means that that nodes can only activate in response to features in the original image of size up to their receptive field. Adding more layers increases the receptive field size and allows the last layer to decide to activate based on more complex patterns.

After the convolutional layers, we also added one fully connected layer with 64 hidden units that was also fully connected to the output to help with classification. The network was trained with stochastic gradient descent with batch size of 10.

\subsubsection{Baseline}
 Our baseline was two layers, both with a filter size of $5$, stride of $2$, and a depth of 16. This achieved a classification accuracy of $~63\%$. 

\subsubsection{Pooling}

\subsubsection{Filter Size and Stride Length}

 We first started experimenting with different filter sizes and stride lengths while keeping both layers the same.

\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
  & Filter = 3 & Filter = 5 & Filter = 9 & Filter = 15 \\ [0.5ex] 
 \hline\hline
 Stride = 1 & 0 & 0 & 0 & 0 \\ 
 \hline
 Stride = 2 & 0 & 0 & 0 & 0 \\
 \hline
 Stride = 3 & 0 & 0 & 0 & 0 \\
 \hline
 Stride = 4 & 0 & 0 & 0 & 0 \\
 \hline
\end{tabular}
\end{center}

 We found that decreasing the stride length to 1 dramatically increased runtime without much gain in accuracy while increasing the stride length could quickly decreased accuracy. Increasing the filter sizes of both layers did not significant improve perfomace past 5 but increased runtime.

Thus for any layer, the optimal filter size and stride length combinations for good performance and runtime were $(3,1)$, $(5,2)$ and $(7,2)$.

\subsubsection{3 Layers}
At first, the jump to 3 layers did not yield any notable improvement. Using three uniform layers, with filter sizes of 5 and depth of 16, 

We then tried different architectures and the results are documented in the table below. Note that a filter size of 5 implies a stride length of 2, etc.

\begin{center}
 \begin{tabular}{||c c c c c c||} 
 \hline
 Layer 1 & Layer 2 & Layer 3 & Classification Accuracy & Training Accuracy & Training Time \\ [0.5ex] 
 \hline\hline
 (5, 16) & (5,16) & (5,16) & 0 & 0 & 40 sec \\ 
 \hline
 (7, 16) & (7,16) & (7,16) & 0 & 0 & 40 sec \\ 
 \hline
 (7, 8)  & (5,16) & (3,32) & 0 & 0 & 40 sec \\ 
 \hline
 (3, 4)  & (5,8) & (7,16) & 0 & 0 & 40 sec \\ 
 \hline
 (3, 8)  & (5,16) & (7,32) & 0 & 0 & 40 sec \\ 
 \hline
\end{tabular}
\end{center}

The uniform architectures took did not significant outperform 2 layer architectures but took much more time to train. One idea was to stack the 

\subsection{Regularization}

Explain dropout.

\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
  & Dropout = 0.5 & Dropout = 0.7 & Dropout = 0.9  & Dropout = 1.0 \\ [0.5ex] 
 \hline\hline
 Classification Accuracy & 0 & 0 & 0 & 0 \\ 
 \hline
 Training Accuracy       & 0 & 0 & 0 & 0 \\
 \hline
\end{tabular}
\end{center}

Dropout not very effective.
Data augmentation only effective


\subsection{Results}
Our optimal CNN architecture is a 3 layer pyramid. Words

\begin{figure}[h]
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data1_test_C1.png}
                \caption{Dataset 1, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data2_test_C1.png}
                \caption{Dataset 2, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data3_test_C1.png}
                \caption{Dataset 3, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data4_test_C1.png}
                \caption{Dataset 4, C=1}
        \end{subfigure}
        \caption{Linear SVM perfomance of different datasets. Datasets 1 and 3 are relatively linearly separable so this SVM performs well on them. Dataset 2 and 4 are definitely not linearly separable so this SVM is not the right model for the data.}\label{fig:animals}
\end{figure}

\vfill
\end{document}

