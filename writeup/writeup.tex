\documentclass[10pt,twoside]{article}

\usepackage[margin=.8in]{geometry}
\usepackage{epsfig}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\title{6.867 Machine Learning  \subtitle{Homework 3} }

\maketitle

% **************************************************************************************************
 % Problem 1
% **************************************************************************************************

\section{\uppercase{Neural Networks}}

\noindent In this paper, we explore the performance of Neural Networks on different classification tasks. Neural Networks were invented to try to model how the neurons work in the human brain. A neural network consists of multiple layers of neurons, each of which takes a linear combination of its inputs and uses that to compute an output. 


\subsection{Implementation}

In our implementation of a neural network, we represented the the weights of each layer as an $m$ x $n$ matrix. Where $m$ is the size of the previous layer and $n$ is the size of the current layer. Each column in the matrix represented the weights for a single neuron in the layer. The bias term for each neuron was captured in a separate $n$ x 1 vector. We used ReLU activation functions for the neurons in the hidden layers and a softmax activation function for the last layer. The loss function we used was the cross entropy loss function, which is $\sum _i -y_i log(f(z)_i) $. Where $f(z)$ is the output of the softmax function. We used stochastic gradient descent to train the parameters of the network, which in this case are the weights and biases of each layer. Computing the gradient of a neural network with respect to the weights is a very complicated operation. Fortunately, the derivative can be found using backpropagation. Backpropagation finds the gradient of the loss function with respect to each layer by propagating the error back through the network. The first step in this is computing the graident of the loss function with respect to the output layer. A convenient property of the combination of a softmax output layer and the cross entropy function is that the derivative of the cost function with respect to the softmax function simplifies to $f(z)-t$ where $f(z)$ is the output of the softmax and $t$ is the target vector. 
\noindent smaller.

\subsection{Section 2}


\subsubsection{sub sub sub section}


\subsection{Install Latex}

% **************************************************************************************************
 % Problem 2
% **************************************************************************************************

\section{\uppercase{Convolutional Neural Networks}}

\noindent Convolutional neural networks(CNNs) are similar to feed forward neural networks but are better suited for the task of image recognition. Traditional neural networks use fully connected layers and hidden units to classify inputs. For even small 50x50 pixel images though, the squared factor of weights between layers that need to be trained quickly makes this problem untractable for any reasonably size dataset.  CNNs solves this problem by recycling parameters with a concept called filters. A filter is a $fxf$ matrix of weights that gets applied to every possible $fxf$ square in the $nxn$ input image to generate another $nxn$ output. Similar to NNs, these outputs then also undergo some nonlinear transformation. Many filters can be applied in parallel to the same input and the output of filters can be fed into the input of other filters. In this manner, CNNs boost a diverse set of possible architectures have made remarkable progress in image recognition.

In this section, we will explore the effect of different filter sizes, architectures, pooling, and regularization techniques on CNNs. The task was classifying images of paintings to their artist.

\subsection{Architecture}

\noindent Paintings were padded to 50x50 pixel RGB images and inputed as a three layer image. We fed these images through various CNNs implemented with TensorFlow. A CNN can consist of multiple layers and each layer takes three arguments, filter size, stride, and depth. Stride is how many pixels the filter shifts each time and is typically 1 or 2. Depth is the number of filters working in parallel at that layer. Note that if a layer has a depth of 8, a filter of size 3 in the next layer actually has $9*8$ nodes feeding into it. The receptive field of a node is defined as the number of pixels in the original image that contribute to it. For example, if a image was fed through a 5x5 filter layer and then a 3x3 filter layer, the receptive field of a node in the last layer would be 7x7. This means that that nodes can only activate in response to features in the original image of size up to their receptive field. Adding more layers increases the receptive field size and allows the last layer to decide to activate based on more complex patterns.

\subsubsection{Baseline}
 Our architecture consisted of one input layer, two convolutional layers, one fully connected layer and one output layer. The fully connected layer contains 64 hidden units and is there to help the output layer with classification. All nodes use RELU activation and output layer uses softmax. The loss function is softmax cross entropy loss ???

\begin{equation*}
hi
\end{equation*}
The network was trained with stochastic gradient descent with batch size of 10.

 In this paper, we only explore the effects of changing the convolutional layers. The baseline contains was two convolutional layers, both with a filter size of $5$, stride of $2$, and a depth of 16. This achieved a classification accuracy of $~63\%$ and a training accuracy of $~97\%$. This means that our model is severely overfitting the training set.

\subsubsection{Pooling}
The ReLu activation function on each node in the CNN is often times not selective enough to isolate the important features in an image. Max pooling is an idea that aims to improve selectivity by amplifying high activation and throwing away low activation. After every convolution layer, we add a pooling layer. Max pooling, like filtering, also acts on a $fxf$ window but outputs not a linear combination of the nodes but the max. In this manner, only the large activations survive. Pooling also has a stride length. We applied the same pooling after all the layers.

\begin{center}
 \begin{tabular}{||c c c c||} 
 \hline
  & Size = 2 & Size = 3 & Size = 4 \\ [0.5ex] 
 \hline\hline
 Stride = 1 & 69.1 & 67.8 & 66.7 \\ 
 \hline
 Stride = 2 & 64.4 & 63.2 & 65.2 \\
 \hline
 Stride = 3 & 50.6 & 54.4 & 54.2 \\
 \hline
 Stride = 4 & 51.7 & 49.6 & 50.2 \\
 \hline
\end{tabular}
\end{center}

Notice that as the stride length increase and the filter size increases, the max pooling becomes more and more aggressive and it throws away more and more data. As you can see in the table above, the classification accuracy gets worse. We found that for two layers, the optimal stride length was 1 and filter size was 2 and it had an accuracy that was a significantly better than our baseline.

\subsubsection{Filter Size and Stride Length}

 Next, we experimented with convolutional layers of different filter sizes and stride lengths. We kept the max pooling at window size 2 and stride 1 and used two identical convolutional layers. The results are summarized in the table below.

\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
  & Filter = 3 & Filter = 5 & Filter = 9 & Filter = 15 \\ [0.5ex] 
 \hline\hline
 Stride = 1 & 0 & 0 & 0 & 0 \\ 
 \hline
 Stride = 2 & 0 & 0 & 0 & 0 \\
 \hline
 Stride = 3 & 0 & 0 & 0 & 0 \\
 \hline
 Stride = 4 & 0 & 0 & 0 & 0 \\
 \hline
\end{tabular}
\end{center}

 As you can see, accuracy was consistantly the best when stride length was 1. The accuracy also seems to improve with filter size but not by much past 5. The more significant tradeoff here was runtime. Small filter sizes and large stride length look significantly less time to train. For filter size 15 and stride 1, the CNN took upwards of 15 minutes. We noticed that there was not much reason to use filter sizes larger than 5 and for large filter sizes, stride lengths of 1 and 2 had roughly the same performance. Thus we decided the optimal filter size and stride length combinations for convolutional layers were $(3,1)$, $(5,2)$ and $(7,2)$.

\subsubsection{3 Layers}
With two layers, the best performance we achieved was only $~68\%$ validation accuracy. Thus, we tried increasing the complexity of our network. We used three convolutional layers as opposed to two and varied their parameters. The results are shown in the table below.
Note that a filter size of 5 implies a stride length of 2, etc.

\begin{center}
 \begin{tabular}{||c c c c c c||} 
 \hline
 Layer 1 & Layer 2 & Layer 3 & Classification Accuracy & Training Accuracy & Training Time \\ [0.5ex] 
 \hline\hline
 (5, 16) & (5,16) & (5,16) & 0 & 0 & 40 sec \\ 
 \hline
 (7, 16) & (7,16) & (7,16) & 0 & 0 & 40 sec \\ 
 \hline
 (7, 8)  & (5,16) & (3,32) & 0 & 0 & 40 sec \\ 
 \hline
 (3, 4)  & (5,8) & (7,16) & 0 & 0 & 40 sec \\ 
 \hline
 (3, 8)  & (5,16) & (7,32) & 0 & 0 & 40 sec \\ 
 \hline
\end{tabular}
\end{center}

Initial tests were not promising. Adding another baseline convolutional layer to the network took more time to train and did not improve the classification accuracy at all. We then tried the pyramid structure for the convolutional layers. We gave the first layer a large filter size
We then experimented with increasing the complexity of the architecture to improve our performance. We tried three convolution layers instead of two and although the network took more time to train, we did not yield any notable improvement. Using three uniform layers with filter sizes of 5 and depths of 16, we achieved a classification accuracy of .

The uniform architectures took did not significantly outperform 2 layer architectures but took much more time to train. One idea was to stack the 

\subsection{Regularization}
Across all architectures, overfitting the training data was a common theme. With no regularization, the CNNs commonly achieved training accuracy of close to $100\%$. Validation accuracy only ever reached the high 60s. 
Explain dropout.

\begin{center}
 \begin{tabular}{||c c c c c||} 
 \hline
  & Dropout = 0.5 & Dropout = 0.7 & Dropout = 0.9  & Dropout = 1.0 \\ [0.5ex] 
 \hline\hline
 Classification Accuracy & 0 & 0 & 0 & 0 \\ 
 \hline
 Training Accuracy       & 0 & 0 & 0 & 0 \\
 \hline
\end{tabular}
\end{center}

Dropout not very effective.
Data augmentation only effective


\subsection{Results}
Our optimal CNN architecture is a 3 layer pyramid. Words

\begin{figure}[h]
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data1_test_C1.png}
                \caption{Dataset 1, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data2_test_C1.png}
                \caption{Dataset 2, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data3_test_C1.png}
                \caption{Dataset 3, C=1}
        \end{subfigure}%
        \begin{subfigure}[b]{0.25\textwidth}
                \centering
                \includegraphics[width=\linewidth]{Figures/P2/svm_data4_test_C1.png}
                \caption{Dataset 4, C=1}
        \end{subfigure}
        \caption{Linear SVM perfomance of different datasets. Datasets 1 and 3 are relatively linearly separable so this SVM performs well on them. Dataset 2 and 4 are definitely not linearly separable so this SVM is not the right model for the data.}\label{fig:animals}
\end{figure}

\vfill
\end{document}

